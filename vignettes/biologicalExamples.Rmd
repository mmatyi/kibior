---
title: "KibioR - Biological examples"
author:
- affiliation: CHU de Québec Research Center, Université Laval, Molecular Medicine
    department, Québec, QC, Canada
  email: regis.ongaro-carcy2@crchudequebec.ulaval.ca
  name: Régis Ongaro-Carcy
date: "`r Sys.Date()`"
output:
  rmarkdown::html_document:
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: yes
#   rmarkdown::html_notebook:
#     highlight: tango
#     number_sections: yes
#     theme: cosmo
#     toc: yes
#     toc_depth: 3
#     toc_float: yes
#   BiocStyle::html_document: default
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{KibioR - Biological examples}  
  %\VignetteEncoding{UTF-8}  

references:
- id: stringdbv11
  title: 'STRING v11: protein-protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets.'
  author:
  - family: Szklarczyk
    given: Damian
  - family: L Gable
    given: Annika
  - family: Lyon
    given: David
  - family: Junge
    given: Alexander
  - family: Wyder
    given: Stefan
  - family: Huerta-Cepas
    given: Jaime
  - family: Simonovic
    given: Milan
  - family: T Doncheva
    given: Nadezhda
  - family: H Morris
    given: John
  - family: Bork
    given: Peer
  - family: J Jensen
    given: Lars
  - family: von Mering
    given: Christian
  container-title: Nucleic Acids Research
  URL: 'https://doi.org/10.1093/nar/gky1131'
  DOI: 10.1093/nar/gky1131
  publisher: Oxford University Press
  volume: 47
  issue: D1
  page: D607–D613
  type: article-journal
  issued:
    year: 2019
    month: 1
    day: 8
---

```{r setup, include = TRUE, echo = FALSE, results = FALSE, message = FALSE, warning = FALSE}
library(knitr)
knitr::opts_chunk$set(include = TRUE, 
                      echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      results = "markup", 
                      collapse = TRUE,
                      cache = FALSE,
                      comment = "##")
library(magrittr)
library(ggplot2)
library(dplyr)
library(stringr)
library(readr)
library(kibior)
```

![KibiorR logo](./img//logo_kibio_r_text.png)


```{r init-kibior-instance}
#> ------------------------------------------------------------
#>                            /!\ 
#> Change this declaration for a custom Elasticsearch instance
#> You MUST HAVE a running and accessible Elasticsearch instance.
#>                            /!\ 
#> ------------------------------------------------------------

#> here, the variable "kc" is an Kibior object 
#> with the following initialization values:
#>    host = "elasticsearch"
#>    port = 9200
#>    user = NULL
#>    pwd = NULL
#>    verbose = FALSE
#> which will bind "kc" to the instance named "docker-cluster"
kc <- Kibior$new(host = "elasticsearch")
```

# Use case description

TODO

We will need to handle *alignement format* (BAM, SAM) and *feature format* (GFF, GTF, BED) for file import.

TODO


# Prepare R environment 

An infinite number of use case can be built as KibioR is versatile. 
We will base ours on Bioconductor known import libraries.


## Requirements

As defined on [Bioconductor website](https://www.bioconductor.org/developers/how-to/commonMethodsAndClasses/), common classes and methods already exist. Depending on what our interest is, these packages are required.

As a short sum up, we summarized here the system dependecies for Bioconductor importation packages:

| File format                   | Packages                      | Required system libraries                                     |
|-------------------------------|-------------------------------|---------------------------------------------------------------|
| FASTA                         | Biostrings                    | `zlib`                                                        |
| BAM                           | Rsamtools, GenomicAlignments  | `zlib`, `libbz2`, `liblzma`, `libxml2`                        |
| GTF, GFF, BED, BigWig, etc.   | rtracklayer                   | `zlib`, `libbz2`, `liblzma`, `libxml2`                        |
| VCF                           | VariantAnnotation             | `zlib`, `libbz2`, `liblzma`, `libxml2`                        |
| FASTQ                         | ShortRead                     | `zlib`, `libbz2`, `liblzma`, `libxml2`, `libpng`, `libjpeg`   |
| Mass Spectro data             | MSnbase                       | `pthreads` (optional), `ncdf`                                 |

For instance, with `Ubuntu systems` (apt based), exact libraries names are:

- `zlib`    : zlib1g-dev
- `libbz2`  : libbz2-dev
- `liblzma` : liblzma-dev
- `libxml2` : libxml2-dev
- `libpng`  : libpng-dev
- `libjpeg` : libjpeg-dev
- `pthreads`: libevent-pthreads-2.0-5
- `ncdf`    : libnetcdf-dev

We need `rtracklayer` package for our example, thus we will be able to import *feature formats* (GTF, GFF, BED).
Since it has `Rsamtools` package as one of its dependencies, we will also be able to import *alignment formats* (BAM, SAM).


## Install

To install linked dependencies, use this command:

```bash
# as root
apt update && apt install -y zlib1g-dev libbz2-dev liblzma-dev libxml2-dev libpng-dev libjpeg-dev
```

Then, install `rtracklayer` in R with:

```{r}
# install requirements
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("rtracklayer")
```


## Test

To test our installation, we can try some small open access files:

```{r test-install}
# Ensembl FTP repository
# Homo Sapiens GFF3 data of GRCh38, chromosome Y (193KB)
url <- "ftp://ftp.ensembl.org/pub/release-99/gff3/homo_sapiens/Homo_sapiens.GRCh38.99.chromosome.Y.gff3.gz"
rtracklayer::import(url)

# ENCODE repository
# from: http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwRepliSeq/
# hg19 BAM file (40MB)
url <- "http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwRepliSeq/wgEncodeUwRepliSeqK562G1AlnRep1.bam"
Rsamtools::scanBAM(url)
```


## Return types

Returned types per package are as followed:

| File format                   | Packages          | Methods                       | Get URL | Returned type after import              |
|-------------------------------|-------------------|-------------------------------|---------|-----------------------------------------|
| FASTA                         | Biostrings        | read{DNA/RNA/AA}StringSet()   | yes     | {DNA/RNA/AA}StringSet                   |
| BAM                           | Rsamtools         | scanBam()                     | yes     | list (per BAM) of list (BAM attributes) |
| SAM, BAM                      | GenomicAlignments | readGAlignment*()             | yes     | GAlignments                             |
| GTF, GFF, BED, BigWig, etc.   | rtracklayer       | import()                      | yes     | GRanges                                 |
| VCF                           | VariantAnnotation | readVcf()                     | yes     |                                         |
| FASTQ                         | ShortRead         | readFastq()                   | no      | ShortReadQ                              | 
| Mass Spectro data             | MSnbase           | read{MS/Mgf}Data()            |         |                                         |

<!-- 
MSnbase files import
https://bioconductor.org/packages/release/bioc/vignettes/MSnbase/inst/doc/v02-MSnbase-io.html#2_data_input 
-->

## Kibior utils

FASTA files:

```{r fasta-parse, eval=FALSE}
# Biostrings DNA import (3Mo)
url_dna <- "ftp://ftp.ensembl.org/pub/release-99/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna_rm.chromosome.Y.fa.gz"
dna_ss <- Biostrings::readDNAStringSet(url_dna)

# Biostrings RNA import (6Mo)
url_rna <- "ftp://ftp.ensembl.org/pub/release-99/fasta/mus_musculus/ncrna/Mus_musculus.GRCm38.ncrna.fa.gz"
rna_ss <- Biostrings::readRNAStringSet(url_rna)

# Biostrings AA import (10Mo)
url_aa <- "ftp://ftp.ensembl.org/pub/release-99/fasta/mus_spretus/pep/Mus_spretus.SPRET_EiJ_v1.pep.all.fa.gz"
aa_ss <- Biostrings::readAAStringSet(url_aa)
```

SAM, BAM files: (40Mo, subsetting it)
wget "http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwRepliSeq/wgEncodeUwRepliSeqK562G1AlnRep1.bam"
samtools view -bs 42.1 wgEncodeUwRepliSeqK562G1AlnRep1.bam > subsampled.bam

```{r bam-parse, eval=FALSE}
# GenomicAlignment
url_bam <- "http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwRepliSeq/wgEncodeUwRepliSeqK562G1AlnRep1.bam"
galign_bam_data <- GenomicAlignments::readGAlignments(url_bam)

# Rsamtols
rsamtools_bam_data <- Rsamtools::scanBam(url_bam)
```

GTF, GFF, BED, etc. files:

```{r feature-parse, eval=FALSE}
# rtacklayer
url_gff <- "ftp://ftp.ensembl.org/pub/release-99/gff3/homo_sapiens/Homo_sapiens.GRCh38.99.chromosome.Y.gff3.gz"
url_bed <- "https://s3.amazonaws.com/bedtools-tutorials/web/cpg.bed" # found here: http://quinlanlab.org/tutorials/bedtools/bedtools.html#setup
rtack_data <- rtracklayer::import(url_gff)
```

VCF files:

```{r vcf-parse, eval=FALSE}
# VariantAnnotation

```

FASTQ files:

```{r fastq-parse, eval=FALSE}
# ShortRead
url <- "http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/NA11843/sequence_read/SRR079411.filt.fastq.gz"
f <- tempfile()
download.file(url, destfile = f)
ShortRead::readFastq(f)
```




# 

<!-- 
metagene2::get_demo_bam_files() %>% lapply(kc$import)
-->

library(magrittr)
devtools::install("metagene2")
devtools::install_github("CharlesJB/FantomEnhancers.hg19")

<!-- Kibior instance -->
kc <- Kibior$new("elasticsearch)

<!-- alignement -->
<!-- bam_files <- metagene2::get_demo_bam_files() -->
<!-- 
from ENCODE : 
https://www.encodeproject.org/experiments/ENCSR000FAJ/ 
-->

t1 <- tempfile(fileext = ".bam")
t2 <- tempfile(fileext = ".bam")
download.file("https://www.encodeproject.org/files/ENCFF000YWV/@@download/ENCFF000YWV.bam", t1)
download.file("https://www.encodeproject.org/files/ENCFF000YWW/@@download/ENCFF000YWW.bam", t2)

<!-- 2Go each -->
<!-- b1 <- Rsamtools::scanBam(t1) -->
<!-- b2 <- Rsamtools::scanBam(t2) -->

<!-- create indexed files -->
i1 <- Rsamtools::indexBam(t1)
i2 <- Rsamtools::indexBam(t2)

<!-- features -->
a549 <- FantomEnhancers.hg19::get_fantom_enhancers_tpm(cell_lines = "A549")
k562 <- FantomEnhancers.hg19::get_fantom_enhancers_tpm(cell_lines = "K562")
<!-- Besoin de mean/sum ? -->

mg <- metagene2::metagene2$new(regions = list(a549, k562), bam_files = c(i1, i2))

<!-- push to es -->
a549 %>% kc$push("enhancers_tpm_a549")
k562 %>% kc$push("enhancers_tpm_k562")

<!-- test de requêtes -->
kc_regions <- kc$search("enhancers_tpm_*", query = "start:[120000 TO 200000]") %>%
    lapply(GenomicRanges::makeGRangesFromDataFrame) 

mg <- metagene$new(regions = kc_regions, bam_files = c(i1, i2))



# Biomart



```r
# install biomaRt package
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("biomaRt")

# connect to a mart and dataset
h <- biomaRt::useMart("ensembl", dataset = "hsapiens_gene_ensembl")

# list attributes
s <- biomaRt::listAttributes(h) %>% dplyr::as_tibble()

# well, too big for us to search relevant ones, we will search
s %>% kc$push("biomart_attributes")

# only stable ids
b <- biomaRt::searchAttributes(h, "stable ID") %>% dplyr::as_tibble()
k <- kc$pull("biomart_attributes", query = '"stable id"')$biomart_attributes

# why is there a difference between those two?
bs <- b %>% dplyr::select(name) %>% unique()
ks <- k %>% dplyr::select(name) %>% unique()
dplyr::setdiff(dplyr::union(ks, bs), dplyr::intersect(ks, bs))

# UCSC was not found with biomaRt search in the description attribute
# whereas KibioR found it
# moreover, we took care of including uppercase into the biomart query.
# If we do not, nothing is found
biomaRt::searchAttributes(h, "stable id") %>% dplyr::as_tibble()

# Limitation on the type of attributes use
# we do not take attribute "pages" into account 
# and our request failed
# try: 
#   biomaRt::getBM(ks$name, mart = h)
# If we want to take all stable id attribute into account
# we have to group them. Unfortunately, if a group contains
# too much attributes, execution will take lots of time and
# will sometimes fail. 
# Here one of the groups has more than 300 attributes.
system.time({
    k %>% 
        dplyr::group_by(page) %>% 
        dplyr::select(name) %>% 
        dplyr::group_split(keep = FALSE) %>% 
        lapply(function(x){ 
            biomaRt::getBM(x$name, mart = h) 
        })
})

# Limiting ourselves to 8 attributes will perform the request.
# Sometimes, it also fails by timeout with and error message like this one:
# Timeout was reached: [www.ensembl.org:80] Operation timed out after 300000 milliseconds with 82712281 bytes received
k %>% 
    dplyr::filter(page == "feature_page") %>%
    dplyr::select(name) %>%
    .[["name"]] %>%
    biomaRt::getBM(mart = h)

# The problem is the quantity of data and it gets exponentially longer
# when asking for more than 2 attributes as combinaison of ids explodes.  
translation_table_attr <- c("ensembl_gene_id", "ensembl_transcript_id")
translation_table_attr %>% 
    biomaRt::getBM(mart = h) %>%
    dplyr::as_tibble() %>%
    kc$push("ens_gene_transcript_ids_tuple")


# # exporting is easy
# s %>% kc$export("./biomart_attributes.csv")

# compare time between elasticsearch and biomart for a simple request
system.time({ 
    biomaRt::searchDatasets(mart = h, pattern = "hsapiens")
})


```

# Examples with biological datasets {#biological-example}

## STRINGdb


<!-- Demo includes reference to STRINGdb
> ***STRING v11: protein-protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets.***
> Szklarczyk D et al., *Nucleic Acids Res*. 2019 Jan; 47:D607-613.
> DOI: [10.1093/nar/gky1131](https://doi.org/10.1093/nar/gky1131) -->

We will use files available from [STRINGdb v11 data](https://string-db.org/cgi/download.pl) [@stringdbv11]:

- the phylogenetic tree of species: [species.v11.0.txt](https://stringdb-static.org/download/species.v11.0.txt),
- the table of presence/absence of orthologous groups in species: [species.mappings.v11.0.txt.gz](https://stringdb-static.org/download/species.mappings.v11.0.txt.gz),
- and the 

```{r stringdb-example-taxon}
#> importing species table data from url
#> since the file is not compressed, we can import it as is.
url <- "https://stringdb-static.org/download/species.v11.0.txt"
phylo_species <- kc$import(url)
names(phylo_species) <- names(phylo_species) %>% 
  gsub("#", "", .) %>%  # remove '#'
  stringr::str_trim()   # removes whitespace
phylo_species
#> push data
s <- kc$push(phylo_species, "stringdb_v11_species", mode = "recreate")
#> search for human
kc$search("stringdb_v11_species", query = "homo sapiens")
```
We can not add some mapping data.

```{r stringdb-example-mapping-slices}
#> Load STRINGdb species mappings file and import it in-memory
#> we could have used the "duplicate_to" attribute of import to send
#> it to Elasticsearch entirely
#> Download from https://stringdb-static.org/download/species.mappings.v11.0.txt.gz
furl <- "https://stringdb-static.org/download/species.mappings.v11.0.txt.gz"
f <- tempfile(fileext = ".tsv.gz")
download.file(url = furl, destfile = f)
#> import data
species_mappings <- kc$import(f)
#> clean names
names(species_mappings) <- names(species_mappings) %>% 
    gsub("#", "", .) %>%  # remove '#'
    stringr::str_trim()   # removes whitespace
species_mappings

#> function: slice-push
sliced_pushes <- function(data, slice_size, indices_prefix = "", push_mode = "recreate", stop_at_slice = NULL){
  #> get the number of slices
  message("Slicing up data...")
  n <- 1:nrow(data)
  slices <- (seq_along(n)/slice_size) %>% 
        ceiling() %>% 
        split(n, .)

  #> iterate over slices 
  for(s in names(slices)){
        #> break if stop_at_slice
        if(!is.null(stop_at_slice) && as.integer(s) > stop_at_slice) break
        #> populate
        index_name <- paste0(indices_prefix, "_p", s)
        sliced_data <- slices[[s]]
        start_index <- sliced_data[[1]]
        end_index <- sliced_data[length(sliced_data)]
        message("Pushing data to '", index_name, "', from rank '", start_index, "' to '", end_index, "'")
        kc$push(data[start_index:end_index,], index_name, mode = push_mode)
  }
}

#> push the three first data slices 
#> starting at the first million
#> cutting on every 100k rows
nb_slices <- 3
size_slice <- 100000
start_at <- 1000000
sliced_pushes(species_mappings[start_at:nrow(species_mappings),], 
              slice_size = size_slice, 
              indices_prefix = "stringdb_v11_species_mappings", 
              stop_at_slice = nb_slices)

#> search for human protein family name from STRINGdb
#> selecting only clusters of orthologous groups (COGs)
s <- kc$search("stringdb_*", 
               query = "species_taxonomy_id:9606 && orthgroup_id:COG*",
               fields = c("species_taxonomy_id", "orthgroup_id", "count"),
               head = FALSE)
s[[1]]
```

```r
# Bioconductor usage boost
# GTF, GFF, BED, BigWig, etc., – rtracklayer::import()
# VCF – VariantAnnotation::readVcf()
# SAM / BAM – Rsamtools::scanBam(), GenomicAlignments::readGAlignment*()
# FASTA – Biostrings::readDNAStringSet()
# FASTQ – ShortRead::readFastq()

# FASTA
BiocManager::install("Biostrings")

# GTF, GFF, etc.
rtracklayer::import()
```



## Leishmania

From uniprot
https://www.uniprot.org/proteomes/UP000000542


ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/reference_proteomes/Eukaryota/UP000000542_5664.fasta.gz



REST: 
https://tritrypdb.org/tritrypdb/serviceList.jsp

```r
# https://tritrypdb.org/webservices/GeneQuestions/GenesByMolecularWeight.json?reference_strains_only=Yes&organism=Leishmania%20braziliensis&min_molecular_weight=0&max_molecular_weight=1000000&o-fields=gene_type,organism

response_type <- "json"
endpoint <- "https://tritrypdb.org/webservices/GeneQuestions/GenesByMolecularWeight"

fields <- c("gene_type","organism")
args <- list("reference_strains_only" = "Yes",
            "organism" = "Leishmania braziliensis",
            "min_molecular_weight" = "0",
            "max_molecular_weight" = "1000000",
            "o-fields" = paste0(fields, collapse = ","))

url <- args %>% 
    purrr::imap(function(x,y){ 
        paste0(y, "=", x)
    }) %>% 
    paste0(collapse = "&") %>%
    paste0(endpoint, ".", response_type, "?", .) %>%
    URLencode()

httr::GET(url, timeout(60))
    
```

# References
